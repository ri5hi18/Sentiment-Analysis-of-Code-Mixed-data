{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.3105 - loss: 1.5038 - val_accuracy: 0.5543 - val_loss: 1.1119\n",
      "Epoch 2/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.5975 - loss: 1.0369 - val_accuracy: 0.6799 - val_loss: 0.8710\n",
      "Epoch 3/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.7112 - loss: 0.8019 - val_accuracy: 0.7191 - val_loss: 0.7674\n",
      "Epoch 4/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7599 - loss: 0.6915 - val_accuracy: 0.7250 - val_loss: 0.7834\n",
      "Epoch 5/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7770 - loss: 0.6378 - val_accuracy: 0.7475 - val_loss: 0.7117\n",
      "Epoch 6/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.7934 - loss: 0.5921 - val_accuracy: 0.7505 - val_loss: 0.7098\n",
      "Epoch 7/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8130 - loss: 0.5549 - val_accuracy: 0.7684 - val_loss: 0.6839\n",
      "Epoch 8/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.8093 - loss: 0.5440 - val_accuracy: 0.7678 - val_loss: 0.6847\n",
      "Epoch 9/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8192 - loss: 0.5337 - val_accuracy: 0.7751 - val_loss: 0.6679\n",
      "Epoch 10/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8279 - loss: 0.5067 - val_accuracy: 0.7650 - val_loss: 0.6906\n",
      "Epoch 11/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8249 - loss: 0.5072 - val_accuracy: 0.7761 - val_loss: 0.6679\n",
      "Epoch 12/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.8366 - loss: 0.4888 - val_accuracy: 0.7724 - val_loss: 0.6703\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7752 - loss: 0.6468\n",
      "Test Accuracy: 77.86%\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Positive       0.90      0.94      0.92      1218\n",
      "      Negative       0.70      0.78      0.73      1257\n",
      "  Not_relevant       0.82      0.75      0.79      1312\n",
      "Mixed Feelings       0.72      0.64      0.68      1238\n",
      "       Neutral       0.76      0.79      0.77      1262\n",
      "\n",
      "      accuracy                           0.78      6287\n",
      "     macro avg       0.78      0.78      0.78      6287\n",
      "  weighted avg       0.78      0.78      0.78      6287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "# Load data from CSV\n",
    "file_path = 'final_manglish_transliterated.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'commentText' column contains strings\n",
    "data['transliterated_text'] = data['transliterated_text'].astype(str)\n",
    "\n",
    "# Function to transliterate Malayalam text to English\n",
    "def transliterate_malayalam_to_english(text):\n",
    "    return transliterate(text, sanscript.MALAYALAM, sanscript.ITRANS)\n",
    "\n",
    "data['transliterated_text'] = data['transliterated_text'].apply(transliterate_malayalam_to_english)\n",
    "\n",
    "# Function to convert sentiment labels to numerical values\n",
    "sentiment_dict = {\n",
    "    'Positive': 0,\n",
    "    'Negative': 1,\n",
    "    'Not_relevant': 2,\n",
    "    'Mixed Feelings': 3,\n",
    "    'Neutral': 4\n",
    "}\n",
    "\n",
    "data['Sentiment_Class'] = data['Sentiment_Class'].map(sentiment_dict)\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "max_words = 1000\n",
    "max_seq_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['transliterated_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data['transliterated_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Prepare target variable\n",
    "labels = np.asarray(data['Sentiment_Class'])\n",
    "\n",
    "# Upsample minority classes to match the size of the majority class\n",
    "data_upsampled = pd.concat([\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Positive']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Negative']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Mixed Feelings']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Neutral']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']]))\n",
    "])\n",
    "\n",
    "# Tokenization and sequence padding for balanced data\n",
    "sequences_upsampled = tokenizer.texts_to_sequences(data_upsampled['transliterated_text'])\n",
    "padded_sequences_upsampled = pad_sequences(sequences_upsampled, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Prepare target variable for upsampled data\n",
    "labels_upsampled = np.asarray(data_upsampled['Sentiment_Class'])\n",
    "\n",
    "# Split the upsampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences_upsampled, labels_upsampled, test_size=0.2, random_state=10)\n",
    "\n",
    "# Build Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=16))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with early stopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original sentiment labels\n",
    "reverse_sentiment_dict = {v: k for k, v in sentiment_dict.items()}\n",
    "y_test_labels = [reverse_sentiment_dict[label] for label in y_test]\n",
    "y_pred_labels = [reverse_sentiment_dict[label] for label in y_pred_classes]\n",
    "\n",
    "# Print complete classification report\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=sentiment_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 68ms/step - accuracy: 0.3237 - loss: 1.4742 - val_accuracy: 0.6256 - val_loss: 0.9481\n",
      "Epoch 2/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 59ms/step - accuracy: 0.7096 - loss: 0.7749 - val_accuracy: 0.8024 - val_loss: 0.6024\n",
      "Epoch 3/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 59ms/step - accuracy: 0.8817 - loss: 0.3933 - val_accuracy: 0.8654 - val_loss: 0.4488\n",
      "Epoch 4/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 59ms/step - accuracy: 0.9542 - loss: 0.1852 - val_accuracy: 0.8843 - val_loss: 0.4111\n",
      "Epoch 5/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 64ms/step - accuracy: 0.9786 - loss: 0.0950 - val_accuracy: 0.8905 - val_loss: 0.4142\n",
      "Epoch 6/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step - accuracy: 0.9868 - loss: 0.0640 - val_accuracy: 0.8976 - val_loss: 0.4112\n",
      "Epoch 7/10\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.9911 - loss: 0.0418 - val_accuracy: 0.8960 - val_loss: 0.4372\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8954 - loss: 0.4434\n",
      "Test Accuracy: 89.72%\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Positive       0.98      0.99      0.99      1295\n",
      "      Negative       0.85      0.91      0.88      1207\n",
      "  Not_relevant       0.90      0.87      0.88      1257\n",
      "Mixed Feelings       0.84      0.81      0.82      1291\n",
      "       Neutral       0.92      0.91      0.91      1237\n",
      "\n",
      "      accuracy                           0.90      6287\n",
      "     macro avg       0.90      0.90      0.90      6287\n",
      "  weighted avg       0.90      0.90      0.90      6287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data from CSV\n",
    "file_path = 'final_manglish_transliterated.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'commentText' column contains strings\n",
    "data['transliterated_text'] = data['transliterated_text'].astype(str)\n",
    "\n",
    "# Function to convert sentiment labels to numerical values\n",
    "sentiment_dict = {\n",
    "    'Positive': 0,\n",
    "    'Negative': 1,\n",
    "    'Not_relevant': 2,\n",
    "    'Mixed Feelings': 3,\n",
    "    'Neutral': 4\n",
    "}\n",
    "\n",
    "data['Sentiment_Class'] = data['Sentiment_Class'].map(sentiment_dict)\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "max_words = 40000\n",
    "max_seq_length = 200\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['transliterated_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data['transliterated_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Prepare target variable\n",
    "labels = np.asarray(data['Sentiment_Class'])\n",
    "\n",
    "# Upsample minority classes to match the size of the majority class\n",
    "data_upsampled = pd.concat([\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Positive']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Negative']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Mixed Feelings']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']])),\n",
    "    resample(data[data['Sentiment_Class'] == sentiment_dict['Neutral']], replace=True, n_samples=len(data[data['Sentiment_Class'] == sentiment_dict['Not_relevant']]))\n",
    "])\n",
    "\n",
    "# Tokenization and sequence padding for balanced data\n",
    "sequences_upsampled = tokenizer.texts_to_sequences(data_upsampled['transliterated_text'])\n",
    "padded_sequences_upsampled = pad_sequences(sequences_upsampled, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Prepare target variable for upsampled data\n",
    "labels_upsampled = np.asarray(data_upsampled['Sentiment_Class'])\n",
    "\n",
    "# Split the upsampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences_upsampled, labels_upsampled, test_size=0.2, random_state=0)\n",
    "\n",
    "e\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with early stopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert numerical labels back to original sentiment labels\n",
    "reverse_sentiment_dict = {v: k for k, v in sentiment_dict.items()}\n",
    "y_test_labels = [reverse_sentiment_dict[label] for label in y_test]\n",
    "y_pred_labels = [reverse_sentiment_dict[label] for label in y_pred_classes]\n",
    "\n",
    "# Print complete classification report\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=sentiment_dict.keys()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
